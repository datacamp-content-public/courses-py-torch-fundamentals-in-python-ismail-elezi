---
title: Insert title here
key: 83c000a6e8fa06235ab589e6835e9684

---
## Activation Functions

```yaml
type: "TitleSlide"
key: "2704723ba4"
```

`@lower_third`

name: Ismail Elezi
title: 


`@script`
In the last lecture we talked about neural networks. We saw how they can be implemented in PyTorch, it looked pretty simple.


---
## Motivation

```yaml
type: "TwoRows"
key: "216e37f9df"
```

`@part1`
![](http://assets.datacamp.com/production/repositories/4019/datasets/035c52d82e1886181135f7342e66172d9b806ed2/1.jpg)


`@part2`
```
input_layer = torch.tensor([2., 1.])
weight_1 = torch.tensor([[0.45, 0.32], [-0.12, 0.29]])
hidden_layer = torch.matmul(input_layer, weight_1)
weight_2 = torch.tensor([[0.48, -0.12], [0.64, 0.91]])
output_layer = torch.matmul(hidden_layer, weight_2)
print(output_layer)
``` {{2}}

```
tensor([ 0.9696,  0.7527])
``` {{3}}


`@script`
Let's look at the neural network drawn here. We see that it has one input layer, one hidden layer and one output layer. We can implement this neural network in PyTorch as we did before, with the code given here, where first we multiply input layer with weight_1, then we multiply their product with weight_2, and in the end we print the results of the output layer, getting two numbers 0.9696 and 0.7527.


---
## Matrix multiplication is a linear transformation

```yaml
type: "TwoRows"
key: "6a4cc1cda6"
```

`@part1`
```
input_layer = torch.tensor([2., 1.])
weight_1 = torch.tensor([[0.45, 0.32], [-0.12, 0.29]])
weight_2 = torch.tensor([[0.48, -0.12], [0.64, 0.91]])
weight = torch.matmul(weight_1, weight_2)
output_layer = torch.matmul(input_layer, weight)
print(output_layer)
print(weight) 
```

```
tensor([ 0.9696,  0.7527]) 
tensor([[ 0.4208,  0.2372], [ 0.1280,  0.2783]])
``` {{2}}


`@part2`
![](http://assets.datacamp.com/production/repositories/4019/datasets/704775bc3e9d17f7295445b333f749913849b981/2.jpg) {{3}}


`@script`
Now, we will try to do something different. Let's first multiply the matrices of weights as we do here with torch.matmul, and then we'll multiply the input with the product of these two matrices. If we print the results, we see something interesting. The result of the output layer is exactly the same as before. This means that we can achieve the exact result by using a single layer neural network, with this particular matrix of weights. 

You may be wondering, is this unexpected? Not really. If you remember from linear algebra, matrix multiplication is actually a linear transformation, meaning that how we have been doing things, we can simplify any neural network in a single layer neural network.


---
## Non linearly separable datasets

```yaml
type: "TwoColumns"
key: "0c2affc121"
```

`@part1`
![](http://assets.datacamp.com/production/repositories/4019/datasets/3ffeb64e2af73497d4b04444a55bfff0da081018/linearly_separable.jpg)


`@part2`
![](http://assets.datacamp.com/production/repositories/4019/datasets/174e614b0dd3d1b867b9437ea30a926d541d0381/linearly_non_separable.jpg){{2}}


`@script`
This comes with an irritating consequence however, our neural nets are not that powerful and using them, we can separate only linearly separable datasets, like the one in the left, essentially datasets that in 2D space can be separated by a straight line, in 3D space by a plane and in a higher dimensional space by a hyperplane. But unfortunately, most datasets actually are not linearly separable, they are messy and look more like the dataset in the right. We need to somehow capture the non-linear relationships between the data, and so we need to build much more powerful models to deal with these datasets.


---
## Activation functions

```yaml
type: "FullSlide"
key: "e276ad2c06"
```

`@part1`
![](http://assets.datacamp.com/production/repositories/4019/datasets/9d726adc5951f256de44005f9c78c3bd632d1f13/activation.jpg)


`@script`
Because of this reason, we use something which is called an activation function. Activation functions are non-linear functions which are inserted in each layer of the neural network, making neural networks nonlinear and allowing them to deal with highly non-linear datasets, thus making them much more powerful.


---
## ReLU activation function

```yaml
type: "TwoRowsTwoColumns"
key: "d902f09f76"
```

`@part1`
![](http://assets.datacamp.com/production/repositories/4019/datasets/3ef5006504e5e02251e382d71b5ad1f90440ebb1/relu.jpg)


`@part2`
`ReLU(x) = max(0, x)`
{{2}}


`@part3`
```
import torch.nn as nn
relu = nn.ReLU()

tensor_1 = torch.tensor([2., -4.])
print(relu(tensor))

tensor_2 = torch.tensor([[2., -4.],
                         [1.2, 0.]])
print(relu(tensor_2))
``` {{3}}


`@part4`
```


tensor([ 2.,  0.]) 


tensor([[ 2.0000,  0.0000],
        [ 1.2000,  0.0000]])
``` {{4}}


`@script`
The most used activation function in neural networks is called ReLU standing for Rectified Linear Unit. Its graph can be seen here and its analytical formula is given by the equation in the right. As you can see, this function accepts a tensor in input and it sets all of its entries that are negative to 0, while it doesn't change the positive entries. Despite being such a simple function, neural networks which use ReLU are extremely powerful, and some of these neural networks are used in complex problems like image recognition, autonomous driving or Alpha Go. 

ReLU is already implemented in PyTorch - as you might expect - and is part of torch.nn submodule. We can see it behavior by looking at the code in the left-lower part <click>, and the results of that code in the other part of the slide.


---
## Let us implement and play with some of the activation functions

```yaml
type: "FinalSlide"
key: "e240ca025e"
```

`@script`
In PyTorch in addition to ReLU there are many other activation functions which are already implemented. Let's start using them and see how they work.

